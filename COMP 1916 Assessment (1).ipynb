import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from pyod.models.knn import KNN
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import precision_recall_curve
# Load the dataset
df = pd.read_csv("creditcard.csv")
# Display the first few rows of the dataset
print(df.head())
# Display dataset information (columns, non-null counts, data types)
print(df.info())
# Display summary statistics
print(df.describe())
# Display the shape of the dataset (number of rows and columns)
print("Dataset Shape:", df.shape)
# Display the column names
print("Column Names:", df.columns.tolist())
# Check for missing values
print("Missing Values:\n", df.isnull().sum())
# Display data types of each column
print("Data Types:\n", df.dtypes)


# Check class distribution in the original dataset
print("Original Class Distribution:\n", df["Class"].value_counts(normalize=True))

# Set sample size 
sample_size = 28480 #10% of the original data set

# Perform stratified sampling to maintain class proportions
df_sample, _ = train_test_split(df, train_size=sample_size, stratify=df["Class"], random_state=42)

# Check class distribution in the sampled dataset
print("Sampled Class Distribution:\n", df_sample["Class"].value_counts(normalize=True))

# Display the first few rows of the sampled dataset
print(df_sample.head())

df_sample['Class'].value_counts()

# Summary statistics
print("Summary Statistics")
print(df_sample.describe())

# Histograms for numerical columns
df_sample.hist(figsize=(12,10), bins=30, edgecolor="black")
plt.suptitle("Feature Distributions")
plt.show()
plt.figure(figsize=(6,4))
sns.countplot(x=df_sample["Class"], palette="coolwarm")
plt.title("Class Distribution (Fraud vs. Non-Fraud)")
plt.xlabel("Class (0 = Non-Fraud, 1 = Fraud)")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(12,8))
sns.heatmap(df_sample.corr(), annot=False, cmap="coolwarm")
plt.title("Feature Correlation Matrix")
plt.show()

# Separate fraud and non-fraud transactions
fraud_data = df_sample[df_sample["Class"] == 1]
non_fraud_data = df_sample[df_sample["Class"] == 0]

# Columns to visualize
features_to_plot = ["Time", "Amount"]

fig, axes = plt.subplots(len(features_to_plot), 1, figsize=(8, len(features_to_plot) * 4))

for i, feature in enumerate(features_to_plot):
    sns.kdeplot(non_fraud_data[feature], label="Non-Fraud", ax=axes[i], fill=True, color="blue", alpha=0.5)
    sns.kdeplot(fraud_data[feature], label="Fraud", ax=axes[i], fill=True, color="red", alpha=0.5)
    axes[i].set_title(f"Anomaly Distribution for {feature}")
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel("Density")
    axes[i].legend()

plt.tight_layout()
plt.show()


# Define features (excluding the target 'class')
X = df_sample.drop(columns=['Class'])

# Scale the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Convert back to DataFrame for easier handling
df_scaled = pd.DataFrame(X_scaled, columns=X.columns)
print("Summary Statistics")
print(df_scaled.describe())
# Fit Isolation Forest model
iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)
df_scaled['anomaly_score'] = iso_forest.fit_predict(X)

# Map predictions: -1 means anomaly (potential fraud), 1 means normal
df_scaled['is_fraud_pred'] = df_scaled['anomaly_score'].apply(lambda x: 1 if x == -1 else 0)

# Check the number of detected anomalies
print("Anomaly counts:\n", df_scaled['is_fraud_pred'].value_counts())

# Display a sample of fraud-labeled transactions
print(df_scaled[df_scaled['is_fraud_pred'] == 1].head())

# Define actual vs. predicted labels
y_true = df_sample['Class']  # Actual fraud labels
y_pred = df_scaled['is_fraud_pred']  # Predicted fraud labels

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Isolation Forest')
plt.show()

# Print classification report
print("Classification Report:\n", classification_report(y_true, y_pred))

# Apply Local Outlier Factor (LOF)
lof = LocalOutlierFactor(n_neighbors=20, contamination='auto')
df_scaled['anomaly_score_lof'] = lof.fit_predict(X)

# Map predictions: -1 means anomaly (potential fraud), 1 means normal
df_scaled['is_fraud_pred_lof'] = df_scaled['anomaly_score_lof'].apply(lambda x: 1 if x == -1 else 0)

# Check the number of detected anomalies
print("Anomaly counts (LOF):\n", df_scaled['is_fraud_pred_lof'].value_counts())

# Display a sample of fraud-labeled transactions
print(df_scaled[df_scaled['is_fraud_pred_lof'] == 1].head())

# Define actual vs. predicted labels
y_true = df_sample['Class']  # Actual fraud labels
y_pred_lof = df_scaled['is_fraud_pred_lof']  # Predicted fraud labels (LOF)

# Compute the confusion matrix
cm_lof = confusion_matrix(y_true, y_pred_lof)

# Plot the confusion matrix
plt.figure(figsize=(6,5))
sns.heatmap(cm_lof, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Local Outlier Factor (LOF)')
plt.show()

# Print classification report
print("Classification Report (LOF):\n", classification_report(y_true, y_pred_lof))

# Apply One-Class SVM
oc_svm = OneClassSVM(kernel='rbf', nu=0.05, gamma='auto')  # nu is the proportion of outliers
df_scaled['anomaly_score_svm'] = oc_svm.fit_predict(X)

# Map predictions: -1 means anomaly (potential fraud), 1 means normal
df_scaled['is_fraud_pred_svm'] = df_scaled['anomaly_score_svm'].apply(lambda x: 1 if x == -1 else 0)

# Check the number of detected anomalies
print("Anomaly counts (One-Class SVM):\n", df_scaled['is_fraud_pred_svm'].value_counts())

# Display a sample of fraud-labeled transactions
print(df_scaled[df_scaled['is_fraud_pred_svm'] == 1].head())

# Define actual vs. predicted labels
y_true = df_sample['Class']  # Actual fraud labels
y_pred_svm = df_scaled['is_fraud_pred_svm']  # Predicted fraud labels (One-Class SVM)

# Compute the confusion matrix
cm_svm = confusion_matrix(y_true, y_pred_svm)

# Plot the confusion matrix
plt.figure(figsize=(6,5))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - One-Class SVM')
plt.show()

# Print classification report
print("Classification Report (One-Class SVM):\n", classification_report(y_true, y_pred_svm))

# Fit KNN model with a fixed contamination rate (adjust based on your dataset)
knn_model = KNN(n_neighbors=5, method='mean', contamination=0.05)  
knn_model.fit(X_scaled)

# Predict anomalies (-1 = anomaly, 1 = normal)
df_scaled['anomaly_score_knn'] = knn_model.predict(X_scaled)

# Convert to binary labels (1 = fraud, 0 = normal)
df_scaled['is_fraud_pred_knn'] = df_scaled['anomaly_score_knn'].apply(lambda x: 1 if x == 1 else 0)

# Count the number of detected anomalies
print("Anomaly counts (KNN):\n", df_scaled['is_fraud_pred_knn'].value_counts())

# Display a sample of fraud-labeled transactions
print(df_scaled[df_scaled['is_fraud_pred_knn'] == 1].head())

# Define actual vs. predicted labels
y_true = df_sample['Class']  # Actual fraud labels
y_pred_knn = df_scaled['is_fraud_pred_knn']  # Predicted fraud labels (KNN)

# Compute the confusion matrix
cm_knn = confusion_matrix(y_true, y_pred_knn)

# Plot the confusion matrix
plt.figure(figsize=(6,5))
sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - KNN')
plt.show()

# Print classification report
print("Classification Report (KNN):\n", classification_report(y_true, y_pred_knn))

# Define Autoencoder model
input_dim = X_scaled.shape[1]  # Number of features
encoding_dim = int(input_dim / 2)  # Compression size

# Build the Autoencoder
input_layer = keras.Input(shape=(input_dim,))
encoded = layers.Dense(encoding_dim, activation='relu')(input_layer)
decoded = layers.Dense(input_dim, activation='sigmoid')(encoded)

autoencoder = keras.Model(input_layer, decoded)

# Compile the model
autoencoder.compile(optimizer='adam', loss='mse')

# Train the Autoencoder
autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, shuffle=True, verbose=1)

# Encode the data
encoded_data = autoencoder.predict(X_scaled)

# Compute reconstruction error
reconstruction_error = np.mean(np.power(X_scaled - encoded_data, 2), axis=1)

# Set threshold for anomaly detection
threshold = np.percentile(reconstruction_error, 95)  # Top 5% as anomalies

# Classify anomalies
df_scaled['is_fraud_pred_autoencoder'] = (reconstruction_error > threshold).astype(int)

# Check the number of detected anomalies
print("Anomaly counts (Autoencoder):\n", df_scaled['is_fraud_pred_autoencoder'].value_counts())

# Display a sample of fraud-labeled transactions
print(df_scaled[df_scaled['is_fraud_pred_autoencoder'] == 1].head())

# Define actual vs. predicted labels
y_true = df_sample['Class']  # Actual fraud labels
y_pred_autoencoder = df_scaled['is_fraud_pred_autoencoder']  # Predicted fraud labels (Autoencoder)

# Compute the confusion matrix
cm_autoencoder = confusion_matrix(y_true, y_pred_autoencoder)

# Plot the confusion matrix
plt.figure(figsize=(6,5))
sns.heatmap(cm_autoencoder, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Autoencoder')
plt.show()

# Print classification report
print("Classification Report (Autoencoder):\n", classification_report(y_true, y_pred_autoencoder))

models = ["Isolation Forest", "LOF", "One-Class SVM", "KNN", "Autoencoder"]
predictions = ["is_fraud_pred", "is_fraud_pred_lof", "is_fraud_pred_svm", "is_fraud_pred_knn", "is_fraud_pred_autoencoder"]

fig, axes = plt.subplots(1, 5, figsize=(20, 5))

for i, (model, pred) in enumerate(zip(models, predictions)):
    cm = confusion_matrix(df_sample["Class"], df_scaled[pred])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'], ax=axes[i])
    axes[i].set_title(f'Confusion Matrix - {model}')
    axes[i].set_xlabel('Predicted')
    axes[i].set_ylabel('Actual')

plt.tight_layout()
plt.show()

plt.figure(figsize=(8,6))

for model, pred in zip(models, predictions):
    precision, recall, _ = precision_recall_curve(df_sample["Class"], df_scaled[pred])
    plt.plot(recall, precision, label=model)

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve for Anomaly Detection Models")
plt.legend()
plt.grid()
plt.show()

# Compute classification reports for all models
models = {
    "Isolation Forest": "is_fraud_pred",
    "LOF": "is_fraud_pred_lof",
    "One-Class SVM": "is_fraud_pred_svm",
    "KNN": "is_fraud_pred_knn",
    "Autoencoder": "is_fraud_pred_autoencoder"
}

# Store results for both Class 0 (Non-Fraud) and Class 1 (Fraud)
model_scores = {model: {} for model in models.keys()}

# Extract precision, recall, and F1-score for both classes (0 and 1)
for model, pred_col in models.items():
    if pred_col in df_scaled.columns:
        report = classification_report(df_sample["Class"], df_scaled[pred_col], output_dict=True)
        model_scores[model]["Precision_0"] = report["0"]["precision"]
        model_scores[model]["Recall_0"] = report["0"]["recall"]
        model_scores[model]["F1_0"] = report["0"]["f1-score"]
        model_scores[model]["Precision_1"] = report["1"]["precision"]
        model_scores[model]["Recall_1"] = report["1"]["recall"]
        model_scores[model]["F1_1"] = report["1"]["f1-score"]
    else:
        print(f"Warning: {model} predictions missing!")

# Convert to NumPy arrays for plotting
metrics = ["Precision", "Recall", "F1-Score"]
x = np.arange(len(metrics))
width = 0.15  # Bar width

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Plot for Class 0 (Non-Fraud)
for i, (model, scores) in enumerate(model_scores.items()):
    axes[0].bar(x + i * width, [scores["Precision_0"], scores["Recall_0"], scores["F1_0"]], width, label=model)

axes[0].set_xlabel("Metric")
axes[0].set_ylabel("Score")
axes[0].set_title("Performance Comparison (Class 0 - Non-Fraud)")
axes[0].set_xticks(x + width)
axes[0].set_xticklabels(metrics)
axes[0].legend()

# Plot for Class 1 (Fraud)
for i, (model, scores) in enumerate(model_scores.items()):
    axes[1].bar(x + i * width, [scores["Precision_1"], scores["Recall_1"], scores["F1_1"]], width, label=model)

axes[1].set_xlabel("Metric")
axes[1].set_ylabel("Score")
axes[1].set_title("Performance Comparison (Class 1 - Fraud)")
axes[1].set_xticks(x + width)
axes[1].set_xticklabels(metrics)
axes[1].legend()

plt.tight_layout()
plt.show()

fraud_counts = {
    "Isolation Forest": df_scaled["is_fraud_pred"].sum(),
    "LOF": df_scaled["is_fraud_pred_lof"].sum(),
    "One-Class SVM": df_scaled["is_fraud_pred_svm"].sum(),
    "KNN": df_scaled["is_fraud_pred_knn"].sum(),
    "Autoencoder": df_scaled["is_fraud_pred_autoencoder"].sum()
    
}

plt.figure(figsize=(8,5))
sns.barplot(x=list(fraud_counts.keys()), y=list(fraud_counts.values()), palette="viridis")
plt.ylabel("Number of Transactions Flagged as Fraud")
plt.xlabel("Model")
plt.title("Anomalies Detected by Each Model")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x=df_sample["Class"], y=reconstruction_error, palette="coolwarm")
plt.xlabel("Actual Class (0 = Non-Fraud, 1 = Fraud)")
plt.ylabel("Reconstruction Error")
plt.title("Autoencoder Reconstruction Error Distribution")
plt.show()
